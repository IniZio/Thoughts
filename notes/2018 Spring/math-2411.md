# MATH 2411

## Introduction

### Set

> A collection of objects

e.g {1, 2, 3} is a set.

#### FAQ

- Is {1, 1, 2, 3} same as {2, 3, 1} ? **YES**

#### Vann's diagram

> Set presented interiors of a region on a plane

#### Rules

0. Order and duplications do not matter

1. Let A, B be sets, A is subset of B if all elements $\in$ A are $\in$ B. i.e. **A $\subset$ B**

  â€‹e.g. A is all integers divisible by 4, B is all integers divisible by 2

2. Two sets A, B are same if A $\subset$ B and B $\subset$ A

3. {} = $\emptyset$ , i.e. Empty set.

4. Set with **1** element is **singleton**

5. Set of all positive integers is N, for integers is Z, for rational numbers is Q, for real numbers is R
   N $\subset$ Z $\subset$ Q $\subset$ R

6. Let A be a set, P(x) is a condition. {x $\in$ A : P(x)} is like ensuring all elements in x satisfies P(x)
   e.g. {x $\in$ Z : there exists y $\in$ Z s.t. x = 2y} => set of all even integers
   e.g. {x $\in$ R: x $\ne$x} => $\emptyset$

7. Relationship between sets
   1. A $\cup$ B is appear in either of them.
   2. A $\cap$ B is appear in both of them.
   3. A $\setminus$ B is appear in A but not in B

8. Let A, B be sets. The Cartesian product of them i.e. A x B is the set collecting ordered pairs (a, b) where a $\in$ A and b $\in$ B

   > (1, 2) $\ne$ (2, 1) as ordered pairs

   e.g. R x R = R^2 is the set of all ordered pairs of real numbers, or the set of all points in the plane

9. Let A be a finite set, `#(A)` means number of elements in A

10. nPr = $\frac{n!}{(n-r)!}$, nCr = $\frac{n!}{(n-r)!r!}$

11. Let A, B be sets. A function f from A to B which is a rule assigning a member in B to each choice of a member in A, then its is written as $f: A \rightarrow B$
    e.g. Let P be set of all people, define m : P -> P s.t. m(x) = biological mother for all x $\in$ P

12. Sample space of an experiment is set of all possible outcomes. e.g. die is rolled, sample space is {1, 2, 3, 4, ,5, 6}

13. An event is a subset of sampe space. {1, 3, 5} is in possibilities of sample space of a die so is event. E.g. Getting {(1, H), (3, H), (5, H)} is the event for throwing a die and a coin to get odd number and head

14. P(A | B) = $\frac{A \cap B}{P(B)}$. Because calculating it is restricting sample space to B

15. Rule of multiplication: If A $\in$ B $\in$ C, then P(C) = P(C | B) P(B | A) P(A)

#### Definitions:

- Let S be sample space of an experiment. **Probability** means a function from a set of all events to [0, 1] s..t. 
  - 1) P(S) = 1 and P($\emptyset$) = 0
  - 2) If A_1, A_2 are pairwise disjoint events (i.e. wont hapen at same time, we call them mutually exclusive), then P(A_1 $\cup$ A_2 ...) = P(A_1) + P(A_2) + .... 
- If P(A | B) = P(A) or P($A \cap B$) = P(A) P(B), that means they are independent
  e.g. Roll a die twice. Let A be event of getting odd number in 1st, B be event of gett odd in 2nd, C be event of getting odd sum
  P(A) = 1/2, P(B) = 1/2, P(C) = 1/2. P(A $\cap$ B) = 1/4  == 1/2 * 1/2 so they are independent. But P(C $\cap$ A $\cap$ B) = 0 != 1/2 * 1/2 * 1/2. A, B, C are independent pairwise, but 3 of them together is dependent
- Let S be sample space of experiment, events S_1, S_2,... form a partition if S_i $\cap$ S_j = $\emptyset$ and S_1 $\cup$ S_2 .... = S
- Let S be sample space of experiment. A random variable is X: S -> R
- Let X: S -> R be a random variable. a, b $\in$ R.
  1) P(X = a) = P({s $\in$ S: X(s) = a})
  2) P(a < X < b) = P({ s $\in$ S: a < X(s) < b })
  3) P(a $\le$ X $\le$ b) = P({s $\in$ S: a $\le$ X(s) $\le$ b})
  4) P(X > a) = P({s $\in$ S: a $\lt$ X(s)})
- Let X be random variable. Cumulative distribution of X is F: R -> R by F(t) = P(X $\le$ t) for all t $\in$ R
- Probability mass function: basically a function listing all possibliites and their corresponding probabilities.
- Cumulative distribution function: like probability mass function but as system of equations
- Discrete random variables: random variables that can only take finitely many possible values or at most countably  many values
- Probabilty density fnction: Area of all possibilities in the graph
- Let n $\in$ N, and 0 < p < 1,  a discrete random variable x follows hte binomial distribution with parameters n, p if its mass func is P(X = k) = (n! / k!) $p^k (1-p)^{n-k}$
  P(x = k) >= 0
  $\sum_{k=0}^n(x =k) = 1$
- A continuous random variable X following "the normal distribution with mean $\lambda$ and stanard deviation > 0"
  If its density is f(x) = $\frac{1}{\sqrt{2\pi}\lambda}e^{-(x-\lambda)^2/2*s.d.^2}$. The normal distribution with mean 0 and s.d. 1 is standard normal distribution
- If x is discrete random variable, then its mass func is sum, if it is continuous then need to integrate from $-\infty$ to $\infty$
- Variance of x is E((E - E(x))^2), $\sqrt{var(x)}$ is standard deviation
- var(aX) = $a^2$var(X). var(X + a) = var(X).
- var(X) = E($x^2$) - E(x)$^2$
  Proof: Var(X) = E($x^2 - 2xE(x) + E(x)^2$)
  = $E(X^2) - 2E(x)E(x) + E(x)^2$

#### Theorems

- **Associative law**:
  1. (A $\cup$ B) $\cup$ C ===  A $\cup$ (B $\cup$ C)
  2. (A $\cap$ B) $\cap$ C ===  A $\cap$ (B $\cap$ C)


- **Distributive law**: let A, B, C be sets

  1. (A $\cup$ B) $\cap$ C = (A $\cap$ C) $\cup$ (B $\cap$ C)
  2. (A $\cap$ B) $\cup$ C = (A $\cup$ C) $\cap$ (B $\cup$ C)

  Proof of (1) :  is a graph everything is painted

  Proof of (2) : is a graph where only overlaps of C with A and B respectively painted

- **De Morgan law**: let A, B, C be sets

  - A $\setminus$ (B $\cup$ C) = (A $\setminus$ B) $\cap$ (A $\setminus$ C)
  - A $\setminus$ (B $\cap$ C) = (A $\setminus$ B) $\cup$(A $\setminus$ C)

  Proof of (1): is a graph where only part of A that is not covered by B nor C is painted

  Proof of (2): is a graph where part of A that is not covered by (B $\cap$ C) is painted

  >  Remark: if A is set of everything in our context, Then A $\setminus$ B will also be denoted by $B^C$, so it is rewritten as (B $\cup$ C)^C = $B^C \cap C^C$ and (B $\cap$ C)^C = $B^C \cup C^C$

- If A, B are disjoint finite sets i.e. A $\cap$ B = $\emptyset$, then #(A $\cap$ B) = #(A) + #(B)
  Proof: very difficult

- **Principle of inclusion-exclusion**: Let A, B be finite sets, then #(A $\cup$ B) = #(A) + #(B) - #(A $\cap$ B)
  Proof: since A and B $\setminus$ A are disjoint, #(A $\cup$ (B $\setminus$ A)) = #(A) + #(B $\setminus$ A) -> #(A $\cup$ B) =#(A) + #(B $\setminus$ A).
  And same for (A $\cap$ B) and B $\setminus$ A, #((A $\cap$ B) $\cup$ (B $\setminus$ A)) = #(A $\cap$ B) + #(B $\setminus$ A) -> #(B) =#(A $\cup$ B) + #(B $\setminus$ A).
  Put (2) in (1) then we get the principle
  e.g. A team has 23 players, 17 can kick with right foot, 8 can kick with left. 
  as corresponded to the principle formula: 23 = 17 + 8 - #(A $\cap$ B), #(A $\cap$ B) = 2
  Also  #(A $\cup$ B $\cup$ C) = #(A $\cap$ B $\cap$ C)

  also #(A$\cup$B$\cup$C) = #(A) + #(B) + #(C) - #(A $\cap$ B) - #(B $\cap$ C) - #(C $\cap$ A) + #(A $\cap$ B $\cap$ C)

- **Principle of multiplication**: If A, B are finite sets, #(A $\times$ B) = #(A) $\cdot$ #(B)
  Proof: By induction on #(B).
  Now if #(B) = 0 i.e. B is empty set, s.t. A x B = empty set. So LHS === RHS
  Suppose the formula is true whenever #(B) = k, and that #(B') = k + 1 i.e. B' is {b} $\in$ B
  for some b $\in$ B' and $\notin$ B s.t. #(B) = k, then #(A x B') = #((A x B) $\cup$ (A x {b}))
  then #(A x B') = #((A x B) $\cup$ (A x {b})) = #(A x B) + #(A x {b}) = #(A) x (#(B) + 1) = #(A) x #(B')

  > If #(A) = n and #(B) = m, then there are n x m ways to choosing one from A and then one from B

- Let S be set of n members, and number of sequences of r distinct objects in S is $n P r$ . If n is a positive integer, then n! = n(n-1)...1 and 0! = 1
  Proof: To create the sequence in S, have n ways to choose an object in S and then (n - 1) ways to choose one again different from the previous, the (n - 2) for third one ..... 

- **Baye's theorem**: If S_1, S_2, ... form a partition, then for each event A, P(A) = $\sum_j $ P(A | S_j)P(S_j)

- A bernowllis trials is an experiment in which only 2 outcoms are possible, they are called success and failure, whose possibilities are p, 1-p. Let this trial be repeated for n times, and let x be number of success obtained.
  P(x = 0) = (1-p)$^n$
  P(x = 1) = p(1-p)$^{n-1} + p (1-p)^{n-2} + ... = n p(1-p)^{n-1}$
  P(x = 2) = n!/2! * $p^2(1-p)^{n-2}$

- $e^x = \sum_{k=0}^\infty \frac{x^k}{k!}$
  backup: $\sum_{k=0}^\infty e^{-x}\frac{\lambda^k}{k!} = e^{-x} \sum_{k=0}^\infty \frac{\lambda^k}{k!} = e^{-\lambda}e^\lambda = 1$

- If n > 0 is large,  p > 0 is small so that np = $\lambda$ is moderate, then (n! / k!) $p^k (1-p)^{n-k} = e^{-\lambda}\frac{\lambda^k}{k!}$
  e.g. Let x be number of goals in a football game, x follows the poisson disribution with parmeter $\lambda$, $\lambda$ = average number of goals in one game
  if one team has one goal in one game in average,
  then probability of it to win i.e. P(x = 2 and y =0) = $e^{-74/28} (74/29)^2 * 1/2! * e^{-1}$ = .1

- If X follows the normal distribution with a mean and s.d., then Y = (x - mean) / s.d. follows the standard normal
  proof: For each t $\in$ R, P(Y $\le$ t) = P($\frac{x - mean}{s.d.} \le t$) = $\int_{-\infty}^{mean + s.d. * t} \frac{1}{\sqrt{2\pi} * s.d.} e^{-(x-mean)^2/2(s.d.)^2}$
  Let y = $\frac{x-mean}{s.d.} $, dy = 1/mean dx. .... x -> -$\infty$, y -> -$\infty$
  = $\int_{-\infty}^t \frac{1}{\sqrt{2\pi}} e^{-y^2/2} dy$ = P(standard normal <= t)

- When n is large, the binomial distribution with parameters n, p is approximated by the normal distribution with mean np and s.d. $\sqrt{np(1-p)}$

- Let X be continuous random variable with density f, g: R -> R be a function, then E(g(X)) = $\int_{-\infty}^{\infty} f(x)g(x) dx$
  Proof: assume g is increasing for simplicity (cuz actually it changes pre sub-interval)
  for each t, P(g(X) <= t) = P(X <= $g^{-1}(t)$) = $\int_{-\infty}^{g^{-1}(t)} f(s) ds$.  (where s = $g^{-1}(t)$)
  The derivative is density of g(X) i.e. $f(g^{-1}(t))(g^{-1})'(t)$
  E(X) = $\int_{-\infty}^{\infty}fg(s)f(s) ds$

- If X is a random variable, then E(aX) = aE(x)

- There are random variables whose expectation is finite but variance is infinite



#### Proof's

###### nCr

To create a sequence of r distinct objects in S:

1st method: there are n!/(n-r)! ways

2nd method: step 1 is choose r distinct objects in S, or choose subset of S consisting of r members. There are n!/(n-r)!r! ways

###### Refer to definition(1), for each A $\in$ S P(S \ A) = 1 - P(A)

Observe A, S\A are mutually exclusive, so by 2), P(A $\cup$ (S\A)) = P(A) + P(S\A), 1 = P(S) = P(A) + P(S\A). So can write S\A as $A^C$

###### Princliple of inclusion-exclusion

Since A, B\A

**Baye's theorem**: P(A) = P(A $\cap$ (S_1 $\cup$ S_2. ...)) = $\sum_j P(A \cap S_j) = \sum_j (P(A | S_j) P(S_j))$ since (A $\cap$ S_i) $\cap$ (A $\cap$ S_j) = $\emptyset$ 

Lemma: Cumulative distribution for random variable X is increasing:
Proof: Let t_1 < t_2, {s $\in$ S: X(s) $\le$ t_1} $\subset$ {s $\in$ S: X(s) $\le$ t_2}, so probability of LHS $\le$ probability of RHS

#### Questions

1. How many different straight flushes are there in poker?
   There are 4 ways to make a suit, 10 ways to make a straight, so #(A) x #(B) = 40 ways

2. How many 4 of a kind are there in poker?
   There are 13 ways to find 4 cards of same value, then 48 ways to find the fifth card, so 13 * 48

3. How many ways to assign 8 players to 5 roles?
   Create a sequence of 5 different players in this squad, so there are 8! / 3! ways

4. How many hands in poker with 5 cards?
   52! / (5! 47!)

5. How many ways to arrange n red balls and m green balls into a sequence?
   First have (n+m)! / (n!) ways to choose n members from all balls
   Then for each j, put red in jth slot whenever j $\in$ R
   Then put m green balls into the remaining slots

6. How many queues of 1, 2... , n so that the number j is located at jth slot for certain j?
   Let A_j be set of all queues in which j is located at te jth slot
   for each j, Target #($A_1 \cup A_2 \cup ... A_n$)  = #($A_1$) + .... #($A_n$) - #($A_1 \cup A_2$) - ... + #($A_1 \cup A_2 \cup A_3$) ...

   | k    | a single term in kth row    | # of terms in the kth row |
   | ---- | --------------------------- | ------------------------- |
   | 1    | #(A_1) = (n-1)!             | n                         |
   | 2    | #(A_1  $\cap$ A_2) = (n-2)! | n(n-1) / 2!               |
   | 3    | (n-3)!                      | n(n-1)(n-2) / 3!          |

   so in the end its n!(1 - 1/2! + 1/3! ... + (-1)^{n-1} / n!), similar to n!(1 - 1/e)

7. Family with 2 kids chosen at random. Find probability that remaining kid is boy, given one of kids in the family chosen is a boy.
   Let A = event that remaining kid is boy = {bb}, B = event that one of the kids is boy = {bb, bg, gb}
   So P(A | B) = 1/3

8. Have n red balls and m green balls, pick 2 balls without replacement, find probability of getting two red balls
   Let A = even getting a red ball in first trial, B = event of getting two red balls in a row
   P(B | A) = $P(A \cap B) / P(A)$ $\rightarrow$ P(B) = P(B|A) P(A)
   since P(A) = n / (n + m) and P(B | A) = (n-1) / (n + m -1) $\Rightarrow$ P(B) = n(n-1) / (n+m)(n+m-1)

9. Probability of getting 4 heads when throwing it n times.
   Let A be getting the 4 heads, S_i be getting i when throwing the die
   P(A) = P(A | S_1) P(S_1) + ... + P(A | S_6) P(S_6) = P(A | S_4) P(S_4) + ... + P(A | S_6) P(S_6) = 1/2^4 * 1/6 + 5/2^5 * 1/6 + 6/2^6 * 1/6

10. Have 1% of defect in a population. If a defect test has 0.9 chance return positive if ppl really have defect, but have 0.1 chance return positive if not really have defect. Probability that the ppl really have defect, given the test returns positive.
  P(B) = P(B | A)P(A) + P(B | A^C)P(A^C) = .9 * .01 + 0.1 * 0.99.
  P(A | B) = P(A $\cap$ B) / P(B) = P(B | A)P(A) / P(B)

11. A die is rolled twice. Sample space is S = {1, 2, ..., 6}$^2$. 
   Define X: S -> R by X((x, y)) = x for all (x, y) $\in$ S. X is a random variable called the number obtained in the 1st trial.
   Deifine Y: S -> R by Y((x, y)) = y for all (x, y) $\in$ S. Y is a random variable called the number obtained in the 2nd trial
   Deifine T: S -> R by T((x, y)) = x+y for all (x, y) $\in$ S. T is a random variable called the sum obtained
   Deifine M: S -> R by Y((x, y)) = max{x, y} for all (x, y) $\in$ S. M is a random variable called  maximum number obtained

12. Coin is flipped 4 times. Define X: S -> R by X((a, b, c, d)) = number of H among them. X is a random variable called number of heads obtained

13. TIme needed to wait for next bus is taken. S = [0, +$\infty$). Define T: S -> R by T(t) = t for all t $\in$ [0, +$\infty$)

14. An insurance company measures the nuber of claims every year. sample space is S = N $\in$ {0}. 
   Define V: S -> R by V(n) = 1000n for all n $\in$ S.V is another variable called the total value of the claims next year, assming each claim is $1000.				

	5. Let S = {1, 2, ..., 6}	 be sample space of the experiment in which the fair die is rolled. calculate P(X < 3.2) where X = the number obtained
   solution: P(X < 3.2) = P({s $\in$ S: X (s) < 3.2}) = P({1, 2, 3}) = 1/2

16. Fair coin is flipped 4 times. S = {H, T}$^4$. X = number of head obtained. get P(0 $\le$ X < 2)
   P(0 $\le$ X < 2) = P({(TTTT), (HTTT), (THTT), (TTHT), (TTTH)}) = 5/16

16. 100 problems with 5 choices. Find probability of scoring < 16 with pure guessing
   Let X be number of problems guessed correctly. X follows the binomial distribution with parameters 100, 1/5.
   P(X <= 15) = (4/5)$^{100}$ + $C_1^{100} (1/5)(4/5) + ... $, which is hard to calculate
   alternatively, X is about normally distibuted with mean np = 100 * 1 / 5 and s.d. $\sqrt{np(1-p)}$ = 4
   P(X <= 15.5) = P($\frac{X - 20}{4} <= \frac{15.5 - 20}{4}$) = P(standard normal <= -1.125) = 0.13

17. An unfair die that gets probabilities of 1/2, 1/4, 1/8, 1/16, 1/32, 1/32 on 1-6
   The expected value of throwing it n times is same as throwing it once

18. Find E(M), the max number of fair die thrown twice
   Let f be mass function of M.
   f(1) = 1 - 0/36, f(2) = (2^2 - 1^2)/36, f(3) = $3^2 - 2^2/36$ ...
   E(M) = 1f(1) + 2f(2) + ...
   = $[1(1^2-0) + 2(2^2 - 1^2) + ...] / 36$ 

19. X is uniformly distributed in [0, 1], find E(X)
   E(X) = $\int_{-\infty}^\infty x f(x) dx = \int_{-\infty}^0 x f(x) dx + \int_0^1 x f(x) dx + \int_1^\infty x f(x) dx$ = 1/2 $x^2$

20. Calculate E(Y) given X is uniformly distributed on [0, 1], and Y = X^2
   For each 0 <= t <= 1, P(Y <= t) = P(X <= $\sqrt{t}$) = $\sqrt{t}$. 
   So density of Y is f(t) = $\frac{1}{2\sqrt{t}}$
   E(Y) = $\int_{-\infty}^{\infty} tf(t) dt$ = $\int_0^1 t/2\sqrt{t}dt$ = 1/3

21. Calculate E(Y) given X is uiformly distributed on [0, 1], and Y = 1/X
   For each t >= 1, P(Y<=t) = P(X >= 1/t) = 1- 1/t
   density of Y is f(t) = 1/t^2
   E(Y) = $\int_1^{+\infty} t dt/t^2$ = $\int_1^{\infty} dt / t$ (diverges to +infinity)

22. Let X be number obtained when a fair die is rolled, calculate E(X^2)
   E(X^2) = $1^2/6 + 2^2/6 + ... 6^2/6$
   = 91/6

23. Let X be continuous random variable with density f(t) =$\frac{1}{\pi} \frac{1}{1+t^2}$. Calculate E(X) and Var(x)
   $\int_{-infty}^{infty}1/\pi \frac{dt}{1+t^2} = 1$
   E(X) = $\int_{-\infty}^{infty} \frac{1}{\pi} \frac{tdt}{1+t^2} = 0$
   var(X) = $\int_{-\infty}^{infty} \frac{1}{\pi} \frac{t^2dt}{1+t^2} $


*Mid-term up to here*

Def: Descriptive statistics is subject of presentating data pictorically in order to understand:

- central tendency of given data
- skewness

Def: Given a set of data $x_1 \le x_2 \le ... x_n$. Given 0 < p < 1, and let m be the integer s.t. m $\le np + 0.5 < m+1$

The p-quantile of this set of date is 1/2 ($x_m + x_{m+1}$)

**Remarks**:

- there are some other formulations of quantiles
- when np + 1/2 $\in N$, then the p-quantile of such data is $X_{np+1/2}$
- the numbers min, lower-quatile, median, upper quatile, max are the 5 number sumary of the set of data
- inter-quartile range means upper quartile - lower quartile

**Def**: box plot of a set of data shows all 5 number summaries

**Def**: histogram is very sensitive to choice of intervals. One may study the cumulative distribution instead of distribution 

**Def**: A unimodal distribution skews to the right (left) if it has a long tail on the right (left)

**Def**: a sample is a set of independent and identically distributed random variables (aka. iid)

e.g. Let X1 be mean temperature at a random date pickedin 2017, let X2 be another. They are iid

**Remark**: two concepts "expectation of a random variable" and "the sample avaerage" are totally differnt.

for instance if X_1, ... X_n is a sample,

E(X_1) is a real number (no randomness), $\frac{1}{n}(X_1 + ... X_n)$ is a random variable.

e.g. Let X be height of a randomly picked person in HK. 

Then E(X) =(my height + other's height )  * 1/7^1000000

In reality it is not a practical way

Instead find X_1, .... X_10 which will be independent random variables identically distributed as X. Then 1/10  of their sm will be the average height of 10 random persons

e.g. S^2 = $\frac{\sum_{j=1}^n (x_j - \overline{x})^2}{n-1}$ is called sample variance

If X, Y are independent normal distributions, then X + Y is normally distibuted as well with mean $u_1 + u_2$ and variance $v_1^2 + v_2^2$

**Lemma**: Let X_1, .. X_n be a random sample following the normal distribution with mean u and variance v^2, then $\overline{X} = \frac{1}{n}(X_1 + ... X_n)$ is alsoo normally distributed with mean  u and variance v^2/n , since var(1/n * (X_1+X_2 + ...X_n))

**Theorem**: Let X_1...X_n be a random sample with E(X_1) = u and Var(X_1) = v^2, if n is large, then $\frac{\frac{X_1+...X_n}{n} - u}{v/\sqrt{n}}$

e.g. $E(X_1^2 + ...X_n^2) = E(X_1^2) + ...E(X_n^2) = n$. so the expectation follows the X^2 distribution with n degrees of freedom

**Def**: chi-square distribution: 1^2+ .. n ^2

**Lemma**: If X_1, ... X_n is a random sample normally distrubted with mean u and variance v^2, then $\frac{1}{v^2} \sum_{j=1}^n (x_j - u)^2$ follows the x^2 distribution with n deg of freedom

question: how about $\frac{1}{v^2} \sum_{j=1}^n (x_j - \overline{x})^2$?
$$
\sum_{j}^n (x_j - \overline{x})^2
= \sum_{j}(x_j- u + u - \overline{x})^2 = \sum_{j}^n (x_j - u)^2 + \sum_{j}^n (\overline{x} - u)^2 - 2\sum_j(x_j - u)(\overline{x} - u) = \sum_j(x_j-u)2 - n(\overline{x} - u)^2
$$
so $\frac{1}{v^2} \sum_{j=1}^n (x_j - \overline{x})^2 + \frac{(\overline{x} + u)^2}{v^2/n} = \frac{1}{v^2}\sum_j(x_j-u)^2$

i.e.e sq of a standard normal = x^2 distribution with n degree of freedom

**Theorem**: If x_1, ... x_n are iids folowing the normal distribution with mean u and variance v^2, thene $\frac{1}{v^2}\sum_{j=1}^n (x_j - \overline{x})^2$

**Def**: Let Z, V be independent random variables following the standard normal, and x^2 distribution with n degrees of freedom resp. Then $\frac{Z}{1/\sqrt{n} \sqrt{V}}$ follows the students' t-distribution

**Theorem:** If X_1, ...X_n is a sample following the normal distribution with unknown mean u and known variance v^2. Then for each 0 < $\alpha$ < 1, an $\alpha$ confidence interval of u is ($\overline{X} - Z_{(1-\alpha)/2}v / \sqrt{n}$, $\overline{X} + Z_{(1-\alpha)/2}v / \sqrt{n}$)

e..g. Let n be large, X_1, ... X_n is a sample with unknown expectation u and known variance v^2.

$\frac{\overline{x} - u}{v/\sqrt{n}}$ is approx. standard normally distributed by the central limit theorem. 

In particular P($-Z_{(1+\alpha)/2}$< $\frac{\overline{x} - u}{v/\sqrt{n}}$ < $Z_{(1+\alpha)/2}$). So P($\overline{X} - Z_{(1-\alpha)/2}v / \sqrt{n}$ < u < $\overline{X} + Z_{(1-\alpha)/2}v / \sqrt{n}$) = $\alpha$

e.g. Let X_1,..., X_n be a sample following the normal distribution with unknown mean u and known variance v^2. Find n so that the width of a 95% confidence interval of u is less than $\epsilon$. 

Solution: width of the confidence interval is $2Z_{0.25}v/\sqrt{n}$ < $\epsilon$. this is done if n > $(\frac{2Z_{0.025}v}{\epsilon})^2$

e.g. Let X_1, ... , X_n be the number of sucesses whn a Bernoulli's trial is repeated. Let p be the unknown probability of success in this trial. find what kind of distribution does it follow.

Solution: x_1 + ... x_n follows the binomial distribution with parameters n and p. When n is large, this is approx the normal distribution with mean np and variance np(1- p). Take away its mean and variance:
$$
\frac{X_1 + ...  + X_n - np}{\sqrt{np(1-p)}} = \frac{\overline{X} - p}{\sqrt{p(1-p)/n}}
$$
it is approx. standard normally distributed.

In particular:
$$
P(Z_{(1-\alpha)/2} < \frac{X-p}{\sqrt{p(1-p)/n}} < Z_{(1-\alpha)/2}) = \alpha
\\
P(\overline{X} - Z_{1-\alpha)/2})\sqrt{\frac{p(1-p)}{n}} < p < \overline{X} + Z_{1-\alpha)/2})\sqrt{\frac{p(1-p)}{n}} = \alpha
$$
replace p with \overline{X}. It is then the confidence interval

e.g. For each $0 \le t \le 1$, t(1-t) = -t^2 + t = -(t - .5)^2 + .25 <= .25

Find n s.t. $2 Z_{\frac{1-\alpha}{2}}\sqrt{\frac{.25}{n}} < \epsilon$ 

when n > $(Z_{(1-x)/2}/\epsilon)^2$, $\epsilon > 2 Z_{\frac{1-\alpha}{2}}\sqrt{\frac{.25}{n}}$ >= $2 Z_{\frac{1-\alpha}{2}}\sqrt{\frac{\overline{X}(1-\overline{X})}{n}}$

e.g. Let X_1, ... X_n be a sample folowing the normal distribution with known mean u and unknown variance v^2.

// this allows knowing the distribution with random variable involved.

$\frac{1}{v^2} \sum_j(X_j - u)^2$ follows the $x^2$ distribution with **n** deg of freedom.

P($X_n^2, \frac{1-\alpha}{2}$< $\frac{1}{v^2} \sum_j(X_j - u)^2$ < $X_n^2 \frac{1-\alpha}{2}$) = $\alpha$. i.e. // the region after the region is $(1-\alpha)/2$

P($\frac{\sum_j(X_j - u)^2}{X_n^2, \frac{1-\alpha}{2}}$ < $\alpha$< $\frac{\sum_j(X_j - u)^2}{X_n^2, \frac{1+\alpha}{2}}$)

this is the interval

e.g. Let X_1, ... X_n be a sample folowing the normal distribution with unknown mean u and unknown variance $\sigma^2$.

$\frac{1}{v^2} \sum_j(X_j - u)^2$ follows the $x^2$ distribution with **(n-1)** deg of freedom.

P($X_{n-1}^2, \frac{1-\alpha}{2}$< $\frac{1}{v^2} \sum_j(X_j - u)^2$ < $X_{n-1}^2 \frac{1-\alpha}{2}$) = $\alpha$.

P($\frac{\sum_j(X_j - \overline{X})^2}{X_{n-1}^2, \frac{1-\alpha}{2}}$ < $\alpha$< $\frac{\sum_j(X_j - \overline{X})^2}{X_{n-1}^2, \frac{1+\alpha}{2}}$)
---
title: Math2111
desc: 
category: Study
tags: [ math2111 ]
date: 2017-05-21
filename: 2017-05-21-Math2111
---

# Summary

**Linear independence:**

Given S = {$\begin{pmatrix}1\\1\end{pmatrix}$, $\begin{pmatrix}1\\-1\end{pmatrix}$}, solve $x_1\begin{pmatrix}1\\1\end{pmatrix}$ + $x_2\begin{pmatrix}1\\-1\end{pmatrix}$ = 0

non-pivot columns $\Rightarrow$ have free variables $\Rightarrow$ L.I.

------

T: $R^n \to R^m$ (m: rows, n: columns)
$R^n$ is domain, $R^m$ is co-domain
T(x)=Ax= L.C of columns of A

onto: each y has at **most** one possible x, L.D.
one-to-one: each y has at **least** one possible x, all rows have pivot position

------

Effect of row exchange to determinant:

- replacement: same
- interchange: det A = - det B
- multiply: det B = k det A

Cramer's rule: $x_i$ = $\frac{det A_i(b)}{det A}$

------

Is subspace if:

- is L.I.
- span B = H

Nul A = the possible x for Ax=0. is empty set if invertible

Basis of A (Row A): do RREF $\to$ get the rows that are not all zeros $\to$, do transpose on them
Basis of Col A: do a transpose $\to$ do the same for basis of Row A

Let A be m$\times$n matrix with p pivot positions. Then it has n-p free variables.
dimension of Nul A = n-p
dimension of Row A = dimension of Col A = dimension of A = p

------

Eigenvector = possible x of (A-$\lambda$I)x = 0.
Subspace E = Nul (A-$\lambda$I) is one of eigenspace of A

For a valid eigenvalue $\lambda$, det(A-$\lambda$I) = 0

Similar: $P^{-1}AP=B$
Diagonalizable: similar to diagonal matrix
Finding P: find the basis for corresponding eigenvalues, then put as column vectors with frequency of their power

------

dist(u, v) = ||u-v||
u $\perp$ v if and only if ||u+v||$^2$ = ||u||$^2$ + ||v||$^2$

$S^\perp$ = (Span S)$^\perp$
(Row A)$^\perp$ = Nul A = ((Col A)$^\perp$)$^T$

weighs of L.C. of orthogonal basis $c_j = \frac{y\cdot u_j}{u_j\cdot u_j}$
proj$_wy$ = $\frac{y\cdot u_1}{u_1\cdot u_1}u_1 + ... \frac{y\cdot u_j}{u_j\cdot u_j}$ for S = {$u_1, ...u_j$}
If S is orthornormal, then proj$_wy$ = SS$^T$y

Best approximation: ||y-$y_1$|| is smallest
dist(y, W) = ||y-proj$_w$y ||

Gram-Schmidt process: for finding orthogonal basis for subspace of $R^n$
For S = {$x_1, ... x_n$},
$v_1 = x_1, v_2 = x_2 - \frac{x_2\cdot v_1}{v_1\cdot v_1}, v_3 = x_3 - \frac{x_3\cdot v_1}{v_1\cdot v_1}v_1 - \frac{x_3\cdot v_2}{v_2\cdot v_2}v_2 ...$
$\to$ {$v_1,...v_n$} is the orthogonal basis

$x_1 = ||v_1||w_1, ... x_n = ||v_n||w_n + (x_n\cdot w_1)w_1 + ... (x_n\cdot w_{n-1})w_{n-1} $
$\to$ A = [$w_1,...w_n$]$\begin{bmatrix}||v_1||&x_2\cdot w_1&...&x_n\cdot w_1\\0&||v_2|| &... &x_n\cdot w_2\\...\end{bmatrix}$ = QR

------

Least square solution: the x that ||b-Ax|| is smallest for Ax=b
$A^TAx = A^Tb$
finding x: x = $R^{-1}Q^Tb$

Nul (A) = Nul ($A^T A$)

Apply least square solution to linear models:
finding a line y = $\beta_0 + \beta_1x$ given some data points is same as solving the solution of each data point put into the equation, so is same as solving
y = X$\beta$ where y is $\begin{pmatrix}y_1\\...\\y_n\end{pmatrix}$, X = $\begin{pmatrix}1&x_1\\...\\1&x_n\end{pmatrix}$, $\beta = \begin{pmatrix}\beta_0\\\beta_1\end{pmatrix}$


---
title: Comp2611
desc: L
category: Study
tags: [ comp2611 ]
date: 2016-11-17
filename: 2016-11-17-Comp2611
---

https://course.cse.ust.hk/comp2611/note/COMP2611_Memory_Fall2016_part1.pdf

### Cache

#### Block placement

###### Mapping

==Direct-mapped (1 way)== : one memory match to one possible cache block

​	Method 1 (most common): ==cache_location = block_address MOD number_of_blocks_in_cache==

​		number of cache blocks (N) is $2^m$

​	Method 2: cache_location = if m cache sets, lower order m bits of the block address

​		can avoid latency of MOD
​		compare the tag i.e. the lower m bits, with the memory addresses to see if the block address is actually in memory

​	Example: `lw $t0, 1200($zero)` with direct-mapped cache

​		block address: 	block containing all addresses 

​						between [byte address/ bytes per block] $\times$ byte per block    & 

​								 [byte address/ bytes per block] $\times$ byte per block + [byte per block + 1]

​		memory address generated by CPU = 1200.

​		block address for byte address 1200 = floor(1200/32)

​		it is sent to cache to look for data

​		by direct mapping, the data block is in entry = floor(1200/32) MOD 8 = 5

​	(P.30) What is cache frames?

​	Disadvantage of DM (P. 34):  for new mapped block to same cache frame, have to kick out existing one instead of using other 			empty cache blocks

Cache size: total size of the cache

Have optimal block size to get min. miss rate with relatively good compromise. Too little then not taking full advantage of locality. Too great then too frequent swap in and out

==Full associative (All ways)==: each cache block can be placed ==anywhere== in cache instead of matching one to one

​	Better than Direct-mapping:

​		No cache conflict $\to$ $\uparrow$ cache hit rate. Still have misses due to size aka. capacity miss

​	Worse than Direct-mapping:

​		Need better hardware and time to find a block in cache

==Set associative (N ways)==: each cache block can be placed certain number of locations in cache. Compromise between DM and FA

Example: 2-way set-associative

​	Associativity = 2

\# of cache sets = cache size / cache block size / ==N==

#### Block identification

To tell which block is in location i.e. is it hit or miss?

Direct mapping: | index || valid || tag || data|

​	==Tag== to store address information. It contain high-order bits not used as index. It is residue after chopping off byte offset and index

​	==valid== to indicate whether cache block has valid data

Example: DM cache with 16 KB f data and 8-word blocks

block size of 8 words  $\to$ $2^9$ blocks

==Tag== has 32 - 9 (the 9 above)  - power of (8 word) for 2 i.e. 5 = 18 bits

==Parallel lookup== means looking at all possible locations in memory and only one will be valid i.e. right one. 

How CPU handle cache miss (P.47): make a request to refill cache and CPU is stalled until 1st level cache is filled. Does not need saving state of registers

> instructions can also be cached

#### Block replacement

Candidates for differrent mapping. 1 for DM, all for FA, N for SA

Strategies:

​	==Random==

​	==Least recently used==: the most AFK block. Costly for high associativity



Tutorial

https://course.cse.ust.hk/comp2611/tutorial/comp2611_tutorial12_Fall2016.pdf

Question 1 (P.3). 

(a) Hit rate = 90 /  100 = 90%

(b) 10% $\times$ 10 + 90% $\times$ 2  = 2.8 clock cycles



==Cache relations==  (p.6)

Told you the cache size ,cache block size, memory address length 

No. of blocks in cache = cache size / cache block size

No. of sets in cache = No. of blocks in cache / No. of cache lines per  block (aka. Cache Associativity)

Total bits required for cache = No. of entries $\times$ size of an entry = Number of blocks $\times$ (cache block size + size of tag + 1 valid bit)



Question 1 (P.9)

(a)

No. of blocks in cache = 4K / 16 = 256

No. of sets in cache =  256 / 1

No. of bits for byte offset : 16 => $2^4$ . So it =  4

No. of bits for index field: 256 => $2^8$, So it = 8

No. of bits for tag field = 32 - 4 - 8 = 20

(b)

No. of bits for cache

= No. of blocks in cache $\times$ No. of bits per block

= 256 $\times$ (1+20+16$\times$ 8)  = 38144



Question 1 (P.14)

Direct mapped cache

==Look at the 16th to 23th bits==

0100 0101: miss

0110 1000: miss

0100 0101: miss

0100 0101: matches the previous one, which means it already is stored in cache. So Hit

0100 0101: still same index,x but different tag, so miss

0100 0101: although is same tag as the 2nd line, but it has already been replaced so Miss



top 12 bits for tag

next 8 bits for index

same index means same data for fetch from address. same tag means same cache address to fetch to?

others are byte offsets



Question 1 (P.17)

| 010  | 111  | 001  | 010 (fetched already) | 001  | 101     | 110     | 110     |
| ---- | ---- | ---- | --------------------- | ---- | ------- | ------- | ------- |
| 010  | 111  | 001  | 010                   | 001  | 101     | 110     | 110     |
|      | 010  | 111  | 001                   | 010  | 001     | 101     | 101     |
|      |      | 010  | 111                   | 001  | 010     | 001     | 001     |
|      |      |      |                       | 111  | 001     | 001     | 001     |
|      |      |      |                       |      | ==111== | ==010== | ==010== |
| M    | M    | M    | H                     | H    | H       | M       |         |